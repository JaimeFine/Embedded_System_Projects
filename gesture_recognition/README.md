# ðŸ¤– SP4: Raspberry Pi Servo & Camera Control

This project controls two servo motors connected to a Raspberry Pi using the `pigpio` library. It simulates expressive head movements like nodding, shaking, and teasingâ€”ideal for embedded robotics, interactive installations, or AI companions.

---

## ðŸ“¦ Requirements

- Raspberry Pi (any model with GPIO support)
- 2x Servo motors
- External power supply for servos (recommended)
- `pigpio` library installed and daemon running
- C++17 or later

---

## ðŸ› ï¸ Setup Instructions

### 1. Install Dependencies

```bash
sudo apt update
sudo apt install pigpio
sudo systemctl start pigpiod
```

### 2. Build the Project

```bash
g++ -std=c++17 -lpigpio -lpthread -o sp4 main.cpp
```

### 3. Run the Program

```bash
./sp4
```

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ main.cpp          # Core logic and command interface
â”œâ”€â”€ README.md         # Project documentation
```

---

## ðŸ§  Code Overview

### `BasicSetups` Class

Handles GPIO initialization and servo movement:

- `setup()` â€” Initializes pigpio and sets servo pins
- `moveServo(pin, pulse)` â€” Sends pulse width to servo with bounds checking
- Gesture methods:
  - `NodUp200`, `NodDown400`, etc.
  - `ShakeLeft200`, `ShakeRight400`, etc.

### Gesture Functions

- `Nodding(bool Words)` â€” Simulates agreement
- `Shaking(bool Words)` â€” Simulates disagreement
- `Teasing()` â€” Runs nod and shake in parallel threads

### Command Handling

- `handleCommands(std::string)` â€” Parses user input and triggers gestures
- Supported commands: `Nod`, `Shake`, `Tease`, `Exit`

---

## ðŸŽ® Usage

After launching, SP4 greets you and awaits input:

```text
Hello, I am SP4, what's up!
Try input Nod, Shake and Tease! For leaving, enter Exit!
```

Example interaction:

```bash
> Nod
Agreed!
Nodding...
```

---

## ðŸ§ª Servo Safety

- Pulse widths are clamped between 500 and 2500 Âµs
- Neutral position is 1300 Âµs (reset after each gesture)
- Final shutdown resets both servos to 1500 Âµs

---

## ðŸ§© Future Enhancements

- Integrate PiCamera for vision-based gestures
- Add voice recognition or speech synthesis
- Expand gesture vocabulary
- Web or mobile interface

---

## ðŸ“„ License

This project is licensed under the MIT License.  
Feel free to modify, distribute, and build upon it.
