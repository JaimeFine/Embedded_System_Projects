src/gesture_capture.py:
from picamera2 import Picamera2
import numpy as np
import cv2
import mediapipe as mp	# Mediapipe for the handtracking

# Initialize Mediapipe hand detection
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# Initialize and configure the PiCamera
picam2 = Picamera2()
picam2.configure(picam2.create_preview_configuration(main={"size": (640, 480), "format": "RGB888"}))
picam2.start()

# Prepare to log hand landmark data
log = []
capture_seconds = 5
frame_rate = 30
target_frames = capture_seconds * frame_rate
print(f"Capturing {target_frames} frames-start movingyour hand")

# Capture and process frames, use different fingers to capture different gestures
while len(log) < target_frames:
	frame = picam2.capture_array()
	frame_rgb = frame
	frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
	results = hands.process(frame_rgb)

	if results.multi_hand_landmarks:
		for hand_landmarks in results.multi_hand_landmarks:
			mp_draw.draw_landmarks(frame_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS)	# Draw landmarks and connections on the frame
			coords = [[lm.x, lm.y] for lm in hand_landmarks.landmark]	# Extract (x, y) coordinates of each landmark
			log.append(coords)
			cv2.putText(frame_bgr, f"Logged: {len(log)}/{target_frames}", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)	# Display logging progress on screen
	else:
		cv2.putText(frame_bgr, "No hand detected", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

	cv2.imshow("Capture", frame_bgr)
	cv2.waitKey(1)

# Save the captured hand landmark data, here in the example is the pinkie, you can adjust the filename as will
np.save("gesture_recognition_pack/5finger.npy",log)
print(f"Saved {len(log)} frames")

picam2.stop()
hands.close()
cv2.destroyAllWindows()

src/gesture_recognition.py:
from picamera2 import Picamera2
import cv2
import mediapipe as mp
import numpy as np
import pickle

model = pickle.load(open("/home/JFan/gesture_recognition_pack/svm_model.pkl", "rb"))	# Loading the trained model

# Initialize Mediapipe for the hand tracking
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# Initialize picamera
picam2 = Picamera2()
picam2.configure(picam2.create_preview_configuration(main={"size": (640, 480), "format": "RGB888"}))
picam2.start()

while True:
	frame = picam2.capture_array()
	frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGRA2RGB)
	results = hands.process(frame_rgb)
	frame_bgr = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)

	# Matching and Recognizing hand gestures
	if results.multi_hand_landmarks:
		for hand_landmarks in results.multi_hand_landmarks:
			mp_draw.draw_landmarks(frame_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS)
			coords = [[lm.x, lm.y] for lm in hand_landmarks.landmark]
			x_test = np.array(coords).flatten()
			pred = model.predict([x_test])[0]
			gestures = {0:"Hand",1:"Fist",2:"Thumb",3:"2Finger",4:"3Finger",5:"4Finger",6:"5Finger"}
			gesture = gestures[pred]
			cv2.putText(frame_bgr, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

	else:
		cv2.putText(frame_bgr, "No hand detected", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

	cv2.imshow("Gestures", frame_bgr)
	if cv2.waitKey(1) & 0xFF == ord('q'):
		break

picam2.stop()
hands.close()
cv2.destroyAllWindows()

src/logging_hand_data.py:
# Here the annotations are skipped, many of the code are similar with the gesture capture file
from picamera2 import Picamera2
import cv2
import mediapipe as mp
import numpy as np

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

picam2 = Picamera2()
picam2.configure(picam2.create_preview_configuration(main={"size": (640, 480), "format": "RGB888"}))
picam2.start()

log = []
while True:
	frame = picam2.capture_array()
	frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGRA2RGB)
	results = hands.process(frame_rgb)
	frame_bgr = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)

	if results.multi_hand_landmarks:
		for hand_landmarks in results.multi_hand_landmarks:
			mp_draw.draw_landmarks(frame_bgr, hand_landmarks, mp_hands.HAND_CONNECTIONS)
			coords = [[lm.x, lm.y] for lm in hand_landmarks.landmark]
			log.append(coords)
			cv2.putText(frame_bgr, f"Logged: {len(log)}", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

			thumb_tip = hand_landmarks.landmark[4]
			cv2.putText(frame_bgr, f"Thumb: ({thumb_tip.x:.2f}, {thumb_tip.y:.2f})", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
	else:
		cv2.putText(frame_bgr, "No hand detected", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

	cv2.imshow("Hands", frame_bgr)
	if cv2.waitKey(1) & 0xFF == ord('q'):
		break

np.save("fist_log.npy", log)
picam2.stop()
hands.close()
cv2.destroyAllWindows()

src/training_program.py:
import numpy as np
from sklearn.svm import SVC
import pickle

hand = np.load("/home/JFan/gesture_recognition_pack/hand.npy")
fist = np.load("/home/JFan/gesture_recognition_pack/fist.npy")
thumb = np.load("/home/JFan/gesture_recognition_pack/thumb.npy")
secfinger = np.load("/home/JFan/gesture_recognition_pack/2finger.npy")
thifinger = np.load("/home/JFan/gesture_recognition_pack/3finger.npy")
foufinger = np.load("/home/JFan/gesture_recognition_pack/4finger.npy")
fiffinger = np.load("/home/JFan/gesture_recognition_pack/5finger.npy")

x = np.vstack([hand,fist,thumb,secfinger,thifinger,foufinger,fiffinger]).reshape(-1, 42)
y = ([0]*len(hand)+[1]*len(fist)+[2]*len(thumb)+[3]*len(secfinger)+[4]*len(thifinger)+[5]*len(foufinger)+[6]*len(fiffinger))

model = SVC(kernel='linear')
model.fit(x, y)
print("Model trained")
pickle.dump(model, open("/home/JFan/gesture_recognition_pack/svm_model.pkl", "wb"))

